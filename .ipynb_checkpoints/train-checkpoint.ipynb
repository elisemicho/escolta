{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import models\n",
    "\n",
    "# Basic model parameters as external flags.\n",
    "FLAGS = None\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Input parameters\n",
    "parser.add_argument(\n",
    "        '--train_dir',\n",
    "        type=str,\n",
    "        default='../../data/tfrecords',\n",
    "        help='Directory with the training data.')\n",
    "parser.add_argument(\n",
    "        '--num_classes',\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help='Number of classes.')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument(\n",
    "        '--model_architecture',\n",
    "        type=str,\n",
    "        default='simple_conv1D',\n",
    "        help='What model architecture to use.')\n",
    "\n",
    "# Training parameters\n",
    "parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help='Initial learning rate.')\n",
    "parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help='Number of epochs to run trainer.')\n",
    "parser.add_argument(\n",
    "        '--batch_size', \n",
    "        type=int, \n",
    "        default=10, \n",
    "        help='Batch size.')\n",
    "parser.add_argument(\n",
    "        '--eval_step_interval',\n",
    "        type=int,\n",
    "        default=400,\n",
    "        help='How often to evaluate the training results.')\n",
    "\n",
    "# Output parameters\n",
    "parser.add_argument(\n",
    "        '--summaries_dir',\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help='Directory for Tensorflow summaries')\n",
    "parser.add_argument(\n",
    "        '--save_step_interval',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Save model checkpoint every save_steps.')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Constants used for dealing with the tfrecords files.\n",
    "TRAIN_FILE = 'train_completesample.tfrecords'\n",
    "VALIDATION_FILE = 'dev_completesample.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    \"\"\"Parses an image and label from the given `serialized_example`.\"\"\"\n",
    "    features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            # Defaults are not specified since both keys are required.\n",
    "            features={\n",
    "                        'labels': tf.FixedLenFeature([], tf.int64),\n",
    "                        'shapes': tf.FixedLenFeature([2], tf.int64),\n",
    "                        'features': tf.VarLenFeature( tf.float32)\n",
    "            })\n",
    "\n",
    "    labels = features['labels']\n",
    "    shapes = features['shapes']\n",
    "    feats = features['features']\n",
    "    print(feats)\n",
    "    shapes = tf.cast(shapes, tf.int32)\n",
    "    feats2d = tf.reshape(feats.values, shapes)\n",
    "    print(feats2d)\n",
    "    return labels, feats2d, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padded_shapes(dataset):\n",
    "    \"\"\"Returns the padded shapes for ``tf.data.Dataset.padded_batch``.\n",
    "    Args:\n",
    "    dataset: The dataset that will be batched with padding.\n",
    "    Returns:\n",
    "    The same structure as ``dataset.output_shapes`` containing the padded\n",
    "    shapes.\n",
    "    \"\"\"\n",
    "    return tf.contrib.framework.nest.map_structure(lambda shape: shape.as_list(), dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_shape(train): \n",
    "    c = 0\n",
    "    filename = os.path.join(FLAGS.train_dir, TRAIN_FILE if train else VALIDATION_FILE)\n",
    "    for record in tf.python_io.tf_record_iterator(filename):\n",
    "        c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs(train, batch_size, num_epochs):\n",
    "    \"\"\"Reads input data num_epochs times.\n",
    "    Args:\n",
    "        train: Selects between the training (True) and validation (False) data.\n",
    "        batch_size: Number of examples per returned batch.\n",
    "        num_epochs: Number of times to read the input data, or 0/None to\n",
    "             train forever.\n",
    "    Returns:\n",
    "        A tuple (feats2d, labels).\n",
    "        This function creates a one_shot_iterator, meaning that it will only iterate\n",
    "        over the dataset once. On the other hand there is no special initialization\n",
    "        required.\n",
    "    \"\"\"\n",
    "    if not num_epochs:\n",
    "        num_epochs = None\n",
    "    filename = os.path.join(FLAGS.train_dir, TRAIN_FILE\n",
    "                                                    if train else VALIDATION_FILE)\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        # TFRecordDataset opens a binary file and reads one record at a time.\n",
    "        # `filename` could also be a list of filenames, which will be read in order.\n",
    "        dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "        # The map transformation takes a function and applies it to every element\n",
    "        # of the dataset.\n",
    "        dataset = dataset.map(decode)\n",
    "        #print(dataset)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        \n",
    "        #dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.padded_batch(batch_size, padded_shapes=get_padded_shapes(dataset))\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        #iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=Tensor(\"ParseSingleExample/Slice_Indices_features:0\", shape=(?, 1), dtype=int64), values=Tensor(\"ParseSingleExample/ParseExample/ParseExample:1\", shape=(?,), dtype=float32), dense_shape=Tensor(\"ParseSingleExample/Squeeze_Shape_features:0\", shape=(1,), dtype=int64))\n",
      "Tensor(\"Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ad3ae4400c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mmodel_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_architecture\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             is_training=True)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Define loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elise/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \"\"\"\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Tensor' object is not iterable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not iterable."
     ]
    }
   ],
   "source": [
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # Set parameters to convey to the model\n",
    "    model_settings = models.prepare_model_settings(FLAGS.num_classes)\n",
    "    \n",
    "    # Input images and labels\n",
    "    label_batch, feat2d_batch, shape_batch = inputs(\n",
    "            train=True, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)\n",
    "    \n",
    "    # Build a Graph that computes predictions from the model\n",
    "    logits, dropout_prob = models.create_model(\n",
    "            feat2d_batch, shape_batch,\n",
    "            model_settings,\n",
    "            FLAGS.model_architecture,\n",
    "            is_training=True)\n",
    "\n",
    "    # Define loss\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(\n",
    "                labels=label_batch, logits=logits)   \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "    \n",
    "    # Define optimizer\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.GradientDescentOptimizer(\n",
    "                FLAGS.learning_rate).minimize(cross_entropy_mean)\n",
    "        \n",
    "    # Define evaluation metrics\n",
    "    predicted_indices = tf.argmax(logits, 1)\n",
    "    correct_prediction = tf.equal(predicted_indices, label_batch)\n",
    "    confusion_matrix = tf.confusion_matrix(\n",
    "            label_batch, predicted_indices, num_classes=FLAGS.num_classes)\n",
    "    evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', evaluation_step)\n",
    "\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    # Merge all the summaries and write them out to directory\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/validation')\n",
    "    \n",
    "    # The op for initializing the variables.\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                                         tf.local_variables_initializer())\n",
    "\n",
    "    \n",
    "    # Create a session for running operations in the Graph.\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables (the trained variables and the\n",
    "        # epoch counter).\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Training loop.\n",
    "        start_step = 1\n",
    "        training_steps_max = FLAGS.batch_size * FLAGS.num_epochs\n",
    "        for training_step in xrange(start_step, training_steps_max + 1):\n",
    "                \n",
    "            #print(sess.run(label_batch))\n",
    "            \"\"\"\n",
    "            print(sess.run(tf.shape(feat2d_batch)))\n",
    "            print(sess.run(tf.shape(label_batch)))\n",
    "            print(sess.run(tf.shape(shape_batch)))\n",
    "            \n",
    "            print(sess.run(tf.shape(feat2d_batch)))\n",
    "            print(sess.run(tf.shape(label_batch)))\n",
    "            print(sess.run(tf.shape(shape_batch)))\n",
    "            \n",
    "            print(sess.run(tf.shape(feat2d_batch)))\n",
    "            print(sess.run(tf.shape(label_batch)))\n",
    "            print(sess.run(tf.shape(shape_batch)))\n",
    "            \"\"\"\n",
    "            # Run one training step of the model:\n",
    "            # Write summary, compute accuracy, compute loss, train model, increment step\n",
    "            train_summary, train_accuracy, cross_entropy_value, _ = sess.run(\n",
    "            [merged_summaries, evaluation_step, cross_entropy_mean, train_step])\n",
    "\n",
    "            # Report\n",
    "            train_writer.add_summary(train_summary, training_step)\n",
    "            tf.logging.info('Step #%d: rate %f, accuracy %.1f%%, cross entropy %f' %\n",
    "                                            (training_step, learning_rate_value, train_accuracy * 100,\n",
    "                                             cross_entropy_value))\n",
    "\n",
    "            # Evaluate on validation data  \n",
    "            if (training_step % FLAGS.eval_step_interval) == 0:\n",
    "                valid_set_size = get_dataset_shape(train=False)\n",
    "                total_accuracy = 0\n",
    "                total_conf_matrix = None\n",
    "                for i in xrange(0, valid_set_size, FLAGS.batch_size):\n",
    "                    # Input images and labels.\n",
    "                    valid_label_batch, valid_feat2d_batch = inputs(\n",
    "                            train=False, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)\n",
    "\n",
    "                    # Run evaluation step and capture training summaries for TensorBoard\n",
    "                    # with the `merged` op.\n",
    "                    validation_summary, validation_accuracy, conf_matrix = sess.run(\n",
    "                            [merged_summaries, evaluation_step, confusion_matrix])\n",
    "\n",
    "                    # Report\n",
    "                    validation_writer.add_summary(validation_summary, training_step)\n",
    "                    batch_size = min(FLAGS.batch_size, valid_set_size - i)\n",
    "                    total_accuracy += (validation_accuracy * batch_size) / valid_set_size\n",
    "                    if total_conf_matrix is None:\n",
    "                        total_conf_matrix = conf_matrix\n",
    "                    else:\n",
    "                        total_conf_matrix += conf_matrix\n",
    "                tf.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "                tf.logging.info('Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "                                                (training_step, total_accuracy * 100, valid_set_size))\n",
    "\n",
    "            # Save the model checkpoint periodically.\n",
    "            if (training_step % FLAGS.save_step_interval == 0 or\n",
    "                training_step == training_steps_max):\n",
    "                checkpoint_path = os.path.join(FLAGS.train_dir,FLAGS.model_architecture + '.ckpt')\n",
    "                tf.logging.info('Saving to \"%s-%d\"', checkpoint_path, training_step)\n",
    "                saver.save(sess, checkpoint_path, global_step=training_step)\n",
    "\n",
    "        print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs,training_steps_max))\n",
    "        \n",
    "        # Testing loop\n",
    "        test_set_size = get_dataset_shape(train=False)\n",
    "        tf.logging.info('test_set_size=%d', test_set_size)\n",
    "        total_accuracy = 0\n",
    "        total_conf_matrix = None\n",
    "        for i in xrange(0, test_set_size, FLAGS.batch_size):\n",
    "            \n",
    "            # Input images and labels.\n",
    "            test_label_batch, test_feat2d_batch = inputs(\n",
    "                    train=False, batch_size=FLAGS.batch_size, num_epochs=FLAGS.num_epochs)\n",
    "\n",
    "            # Run evaluation step\n",
    "            test_accuracy, conf_matrix = sess.run(\n",
    "                    [evaluation_step, confusion_matrix])\n",
    "\n",
    "            # Report \n",
    "            batch_size = min(FLAGS.batch_size, test_set_size - i)\n",
    "            total_accuracy += (test_accuracy * batch_size) / test_set_size\n",
    "            if total_conf_matrix is None:\n",
    "                total_conf_matrix = conf_matrix\n",
    "            else:\n",
    "                total_conf_matrix += conf_matrix\n",
    "        tf.logging.info('Confusion Matrix:\\n %s' % (total_conf_matrix))\n",
    "        tf.logging.info('Final test accuracy = %.1f%% (N=%d)' % (total_accuracy * 100,test_set_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
